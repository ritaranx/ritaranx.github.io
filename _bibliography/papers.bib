@article{xu2023knowledge,
  title={Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models},
  author={Xu, Ran and Cui, Hejie and Yu, Yue and Kan, Xuan and Shi, Wenqi and Zhuang, Yuchen and Jin, Wei and Ho, Joyce and Yang, Carl},
  journal={arXiv preprint arXiv:2311.00287},
  year={2024},
  selected={false},
  arxiv={2311.00287},
  code={https://github.com/ritaranx/ClinGen}
}


@inproceedings{xu2023weakly,
  abbr={SIGIR},
  title={Weakly-supervised Scientific Document Classification via Retrieval-Augmented Multi-stage Training},
  author={Xu*, Ran  and Yu*, Yue and Ho, Joyce C and Yang, Carl},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval}, 
  selected={true},
  year={2023},
  arxiv={2306.07193},
  code={https://github.com/ritaranx/WANDER},
}

@inproceedings{xu2023nest,
  abbr={AAAI},
  title={Neighborhood-regularized Self-Training for Learning with Few Labels},
  author={Xu, Ran and Yu, Yue and Cui, Hejie and Kan, Xuan and Zhu, Yanqiao and Ho, Joyce and Zhang, Chao and Yang, Carl},
  booktitle={Proceedings of the 37th AAAI Conference on Artificial Intelligence}, 
  selected={true},
  year={2023},
  pdf = {https://ojs.aaai.org/index.php/AAAI/article/view/26260/26032},
  code={https://github.com/ritaranx/NeST},
  arxiv={2301.03726},
  oral={true},
}

@article{xu2023hypergraph,
  abbr={AMIA},
  title={Hypergraph Transformers for EHR-based Clinical Predictions},
  author={Xu, Ran and Ali, Mohammed K and Ho, Joyce C and Yang, Carl},
  journal={AMIA Summits on Translational Science Proceedings},
  volume={2023},
  pages={582},
  year={2023},
  publisher={American Medical Informatics Association},
  pdf={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10283128/},
  oral={true},
}

@inproceedings{cui2023open,
  abbr={NeurIPS},
  title={Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting},
  author={Cui, Hejie and Fang, Xinyu and Zhang, Zihan and Xu, Ran and Kan, Xuan and Liu, Xin and Yu, Yue and Li, Manling and Song, Yangqiu and Yang, Carl},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  selected={false},
  arxiv={2311.00287},
  pdf={https://openreview.net/forum?id=ixVAXsdtJO},
}

@inproceedings{yu-etal-2023-cold,
   abbr={ACL},
    title = "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach",
    author = "Yu, Yue  and
      Zhang, Rongzhi  and
      Xu, Ran  and
      Zhang, Jieyu  and
      Shen, Jiaming  and
      Zhang, Chao",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.141",
    doi = "10.18653/v1/2023.acl-long.141",
    pages = "2499--2521",
    abstract = "We present PATRON, a prompt-based data selection method for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9{\%}. Besides, with 128 labels only, PATRON achieves 91.0{\%} and 92.1{\%} of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON will be published upon acceptance.",
    pdf = {https://aclanthology.org/2023.acl-long.141.pdf},
    code={https://github.com/yueyu1030/Patron},
}
@inproceedings{kan2023rmixup,
  abbr={KDD},
  title={R-Mixup: Riemannian Mixup for Biological Networks},
  author={Xuan Kan and Zimu Li and Hejie Cui and Yue Yu and Ran Xu and Shaojun Yu and Zilong Zhang and Ying Guo and Carl Yang},
  booktitle={Proceedings of the 29th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  selected={false},
  year={2023},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3580305.3599483},
  arxiv={2306.02532},
}


@inproceedings{yu2022causal,
  abbr={ISBI},
  title={Deep DAG Learning on Brain Networks for fMRI Analysis},
  author={Yu, Yue and Kan, Xuan and Cui, Hejie and Xu, Ran and Zheng, Yujia and Song, Xiangchen and Zhu, Yanqiao and Zhang, Kun and Nabi, Razieh and Guo, Ying and Zhang, Chao and Yang, Carl},
  booktitle={Proceedings of the 20th IEEE International Symposium on Biomedical Imaging}, 
  selected={false},
  year={2023},
  arxiv={2211.00261},
  pdf = {https://ieeexplore.ieee.org/abstract/document/10230429}
}


@inproceedings{cui2023a,
title={A Survey on Knowledge Graphs for Healthcare: Resources, Application Progress, and Promise},
author={Hejie Cui and Jiaying Lu and Shiyu Wang and Ran Xu and Wenjing Ma and Shaojun Yu and Yue Yu and Xuan Kan and Tianfan Fu and Chen Ling and Joyce Ho and Fei Wang and Carl Yang},
booktitle={ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH)},
abbr={ICML (IMLH Workshop)},
year={2023},
pdf={https://openreview.net/forum?id=CZCktJoBRh},
arxiv={2306.04802}
}

@InProceedings{pmlr-v193-xu22a,
  abbr={ML4H},
  selected={true},
  title = 	 {Counterfactual and Factual Reasoning over Hypergraphs for Interpretable Clinical Predictions on EHR},
  author =       {Xu, Ran and Yu, Yue and Zhang, Chao and Ali, Mohammed K and Ho, Joyce C and Yang, Carl},
  booktitle = 	 {Proceedings of the 2nd Machine Learning for Health symposium},
  pages = 	 {259--278},
  year = 	 {2022},
  editor = 	 {Parziale, Antonio and Agrawal, Monica and Joshi, Shalmali and Chen, Irene Y. and Tang, Shengpu and Oala, Luis and Subbaswamy, Adarsh},
  volume = 	 {193},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28 Nov},
  publisher =    {PMLR},
  best={true},
  pdf = 	 {https://proceedings.mlr.press/v193/xu22a/xu22a.pdf},
  url = 	 {https://proceedings.mlr.press/v193/xu22a.html},
  code = {https://github.com/ritaranx/CACHE},
  abstract = 	 {Electronic Health Record modeling is crucial for digital medicine. However, existing models ignore higher-order interactions among medical codes and their causal relations towards downstream clinical predictions. To address such limitations, we propose a novel framework CACHE, to provide <em>effective</em> and <em>insightful</em> clinical predictions based on hypergraph representation learning and counterfactual and factual reasoning techniques. Experiments on two real EHR datasets show the superior performance of CACHE. Case studies with a domain expert illustrate a preferred capability of CACHE in generating clinically meaningful interpretations towards the correct predictions.}
}


}