@article{xu2024simrag,
  abbr={preprint},
  title={SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains},
  author={Xu, Ran and Liu, Hui and Nag, Sreyashi and Dai, Zhenwei and Xie, Yaochen and Tang, Xianfeng and Luo, Chen and Li, Yang and Ho, Joyce C and Yang, Carl and He, Qi},
  journal={arXiv preprint arXiv:2410.17952},
  year={2024},
  selected={true},
  arxiv={2410.17952}
}

@article{shen2024boosting,
  abbr={preprint},
  title={Boosting reward model with preference-conditional multi-aspect synthetic data generation},
  author={Shen, Jiaming and Xu, Ran and Jun, Yennie and Qin, Zhen and Liu, Tianqi and Yang, Carl and Liang, Yi and Baumgartner, Simon and Bendersky, Michael},
  journal={arXiv preprint arXiv:2407.16008},
  year={2024},
  arxiv={2407.16008}
}

@inproceedings{xu-etal-2024-bmretriever,
    abbr={EMNLP},
    title = "{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers",
    author = "Xu*, Ran  and
      Shi*, Wenqi  and
      Yu*, Yue  and
      Zhuang, Yuchen  and
      Zhu, Yanqiao  and
      Wang, May Dongmei  and
      Ho, Joyce C.  and
      Zhang, Chao  and
      Yang, Carl",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1241",
    pages = "22234--22254",
    abstract = "Developing effective biomedical retrieval models is important for excelling at knowledge-intensive biomedical tasks but still challenging due to the lack of sufficient publicly annotated biomedical data and computational resources. We present BMRetriever, a series of dense retrievers for enhancing biomedical retrieval via unsupervised pre-training on large biomedical corpora, followed by instruction fine-tuning on a combination of labeled datasets and synthetic pairs. Experiments on 5 biomedical tasks across 11 datasets verify BMRetriever{'}s efficacy on various biomedical applications. BMRetriever also exhibits strong parameter efficiency, with the 410M variant outperforming baselines up to 11.7 times larger, and the 2B variant matching the performance of models with over 5B parameters. The training data and model checkpoints are released at https://huggingface.co/BMRetriever to ensure transparency, reproducibility, and application to new domains.",
    code={https://github.com/ritaranx/BMRetriever},
    selected={true},
    arxiv={2404.18443}
}


@inproceedings{shi-etal-2024-medadapter,
    abbr={EMNLP},
    title = "{M}ed{A}dapter: Efficient Test-Time Adaptation of Large Language Models Towards Medical Reasoning",
    author = "Shi*, Wenqi  and
      Xu*, Ran  and
      Zhuang, Yuchen  and
      Yu, Yue  and
      Sun, Haotian  and
      Wu, Hang  and
      Yang, Carl  and
      Wang, May Dongmei",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1244",
    pages = "22294--22314",
    abstract = "Despite their improved capabilities in generation and reasoning, adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and privacy concerns. In this study, we propose MedAdapter, a unified post-hoc adapter for test-time adaptation of LLMs towards biomedical applications. Instead of fine-tuning the entire LLM, MedAdapter effectively adapts the original model by fine-tuning only a small BERT-sized adapter to rank candidate solutions generated by LLMs. Experiments on four biomedical tasks across eight datasets demonstrate that MedAdapter effectively adapts both white-box and black-box LLMs in biomedical reasoning, achieving average performance improvements of 18.24{\%} and 10.96{\%}, respectively, without requiring extensive computational resources or sharing data with third parties. MedAdapter also yields enhanced performance when combined with train-time adaptation, highlighting a flexible and complementary solution to existing adaptation methods. Faced with the challenges of balancing model performance, computational resources, and data privacy, MedAdapter provides an efficient, privacy-preserving, cost-effective, and transparent solution for adapting LLMs to the biomedical domain.",
    code={https://github.com/wshi83/MedAdapter},
    arxiv={2405.03000},
}

@inproceedings{shi-etal-2024-ehragent,
    abbr={EMNLP},
    title = "{EHRA}gent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
    author = "Shi*, Wenqi  and
      Xu*, Ran  and
      Zhuang, Yuchen  and
      Yu, Yue  and
      Zhang, Jieyu  and
      Wu, Hang  and
      Zhu, Yuanda  and
      Ho, Joyce C.  and
      Yang, Carl  and
      Wang, May Dongmei",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1245",
    pages = "22315--22339",
    abstract = "Clinicians often rely on data engineers to retrieve complex patient information from electronic health record (EHR) systems, a process that is both inefficient and time-consuming. We propose EHRAgent, a large language model (LLM) agent empowered with accumulative domain knowledge and robust coding capability. EHRAgent enables autonomous code generation and execution to facilitate clinicians in directly interacting with EHRs using natural language. Specifically, we formulate a multi-tabular reasoning task based on EHRs as a tool-use planning process, efficiently decomposing a complex task into a sequence of manageable actions with external toolsets. We first inject relevant medical information to enable EHRAgent to effectively reason about the given query, identifying and extracting the required records from the appropriate tables. By integrating interactive coding and execution feedback, EHRAgent then effectively learns from error messages and iteratively improves its originally generated code. Experiments on three real-world EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6{\%} in success rate, verifying its strong capacity to tackle complex clinical tasks with minimal demonstrations.",
    code={https://github.com/wshi83/EHRAgent},
    arxiv={2401.07128}
}


@inproceedings{xu-etal-2024-knowledge,
    abbr={ACL (Findings)},
    title = "Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models",
    author = "Xu, Ran  and
      Cui, Hejie  and
      Yu, Yue  and
      Kan, Xuan  and
      Shi, Wenqi  and
      Zhuang, Yuchen  and
      Wang, May Dongmei  and
      Jin, Wei  and
      Ho, Joyce  and
      Yang, Carl",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.916",
    doi = "10.18653/v1/2024.findings-acl.916",
    pages = "15496--15523",
    abstract = "Clinical natural language processing faces challenges like complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation with LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 8 clinical NLP tasks and 18 datasets reveals that ClinGen consistently enhances performance across various tasks by 7.7{\%}-8.7{\%} on average, effectively aligning the distribution of real datasets and enriching the diversity of generated training instances.",
    code={https://github.com/ritaranx/ClinGen},
    arxiv={2311.00287}
}

@inproceedings{xu-etal-2024-ram,
    title = "{RAM}-{EHR}: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records",
    abbr={ACL},
    author = "Xu, Ran  and
      Shi, Wenqi  and
      Yu, Yue  and
      Zhuang, Yuchen  and
      Jin, Bowen  and
      Wang, May Dongmei  and
      Ho, Joyce  and
      Yang, Carl",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-short.68",
    doi = "10.18653/v1/2024.acl-short.68",
    pages = "754--765",
    abstract = "We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical predictions on Electronic Health Records (EHRs). RAM-EHR first collects multiple knowledge sources, converts them into text format, and uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts. RAM-EHR then augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on two EHR datasets show the efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4{\%} gain in AUROC and 7.2{\%} gain in AUPR), emphasizing the effectiveness of the summarized knowledge from RAM-EHR for clinical prediction tasks.",
    selected={true},
    oral={true},
    code={https://github.com/ritaranx/RAM-EHR}
}


@InProceedings{pmlr-v248-xu24a,
  title =    {From Basic to Extra Features: Hypergraph Transformer Pretrain-then-Finetuning for Balanced Clinical Predictions on EHR},
  abbr = {CHIL},
  uthor =       {Xu, Ran and Lu, Yiwen and Liu, Chang and Chen, Yong and Sun, Yan and Hu, Xiao and Ho, Joyce C and Yang, Carl},
  booktitle =    {Proceedings of the fifth Conference on Health, Inference, and Learning},
  pages =    {182--197},
  year =   {2024},
  editor =   {Pollard, Tom and Choi, Edward and Singhal, Pankhuri and Hughes, Michael and Sizikova, Elena and Mortazavi, Bobak and Chen, Irene and Wang, Fei and Sarker, Tasmie and McDermott, Matthew and Ghassemi, Marzyeh},
  volume =   {248},
  series =   {Proceedings of Machine Learning Research},
  month =    {27--28 Jun},
  publisher =    {PMLR},
  pdf =    {https://raw.githubusercontent.com/mlresearch/v248/main/assets/xu24a/xu24a.pdf},
  url =    {https://proceedings.mlr.press/v248/xu24a.html},
  abstract =   {Electronic Health Records (EHRs) contain rich patient information and are crucial for clinical research and practice.  In recent years, deep learning models have been applied to EHRs, but they often rely on massive features, which may not be readily available for all patients. We propose \ours{}\footnote{Short for \textbf{H}ypergraph \textbf{T}ransformer \textbf{P}retrain-then-Finetuning with \textbf{S}moo\textbf{t}hness-induced regularization \textbf{a}nd \textbf{R}eweighting.}, which leverages hypergraph structures with a pretrain-then-finetune framework for modeling EHR data, enabling seamless integration of additional features.  Additionally, we design two techniques, namely (1) \emph{Smoothness-inducing Regularization} and (2) \emph{Group-balanced Reweighting}, to enhance the model’s robustness during finetuning. Through experiments conducted on two real EHR datasets, we demonstrate that \ours{} consistently outperforms various baselines while striking a balance between patients with basic and extra features.}
}


@inproceedings{zhang2024tacco,
  abbr={KDD},
  title={TACCO: Task-guided Co-clustering of Clinical Concepts and Patient Visits for Disease Subtyping based on EHR Data},
  author={Zhang, Ziyang and Cui, Hejie and Xu, Ran and Xie, Yuzhang and Ho, Joyce C and Yang, Carl},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={6324--6334},
  year={2024}
}

@inproceedings{xu2023weakly,
  abbr={SIGIR},
  title={Weakly-supervised Scientific Document Classification via Retrieval-Augmented Multi-stage Training},
  author={Xu*, Ran  and Yu*, Yue and Ho, Joyce C and Yang, Carl},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval}, 
  year={2023},
  arxiv={2306.07193},
  code={https://github.com/ritaranx/WANDER},
}

@inproceedings{xu2023nest,
  abbr={AAAI},
  title={Neighborhood-regularized Self-Training for Learning with Few Labels},
  author={Xu, Ran and Yu, Yue and Cui, Hejie and Kan, Xuan and Zhu, Yanqiao and Ho, Joyce and Zhang, Chao and Yang, Carl},
  booktitle={Proceedings of the 37th AAAI Conference on Artificial Intelligence}, 
  selected={true},
  year={2023},
  pdf = {https://ojs.aaai.org/index.php/AAAI/article/view/26260/26032},
  code={https://github.com/ritaranx/NeST},
  arxiv={2301.03726},
  oral={true},
}

@article{xu2023hypergraph,
  abbr={AMIA},
  title={Hypergraph Transformers for EHR-based Clinical Predictions},
  author={Xu, Ran and Ali, Mohammed K and Ho, Joyce C and Yang, Carl},
  journal={AMIA Summits on Translational Science Proceedings},
  volume={2023},
  pages={582},
  year={2023},
  publisher={American Medical Informatics Association},
  pdf={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10283128/},
  oral={true},
}

@inproceedings{cui2023open,
  abbr={NeurIPS},
  title={Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting},
  author={Cui, Hejie and Fang, Xinyu and Zhang, Zihan and Xu, Ran and Kan, Xuan and Liu, Xin and Yu, Yue and Li, Manling and Song, Yangqiu and Yang, Carl},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  selected={false},
  arxiv={2311.00287},
  pdf={https://openreview.net/forum?id=ixVAXsdtJO},
}

@inproceedings{yu-etal-2023-cold,
   abbr={ACL},
    title = "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach",
    author = "Yu, Yue  and
      Zhang, Rongzhi  and
      Xu, Ran  and
      Zhang, Jieyu  and
      Shen, Jiaming  and
      Zhang, Chao",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.141",
    doi = "10.18653/v1/2023.acl-long.141",
    pages = "2499--2521",
    abstract = "We present PATRON, a prompt-based data selection method for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9{\%}. Besides, with 128 labels only, PATRON achieves 91.0{\%} and 92.1{\%} of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON will be published upon acceptance.",
    pdf = {https://aclanthology.org/2023.acl-long.141.pdf},
    code={https://github.com/yueyu1030/Patron},
}
@inproceedings{kan2023rmixup,
  abbr={KDD},
  title={R-Mixup: Riemannian Mixup for Biological Networks},
  author={Xuan Kan and Zimu Li and Hejie Cui and Yue Yu and Ran Xu and Shaojun Yu and Zilong Zhang and Ying Guo and Carl Yang},
  booktitle={Proceedings of the 29th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  selected={false},
  year={2023},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3580305.3599483},
  arxiv={2306.02532},
}


@inproceedings{yu2022causal,
  abbr={ISBI},
  title={Deep DAG Learning on Brain Networks for fMRI Analysis},
  author={Yu, Yue and Kan, Xuan and Cui, Hejie and Xu, Ran and Zheng, Yujia and Song, Xiangchen and Zhu, Yanqiao and Zhang, Kun and Nabi, Razieh and Guo, Ying and Zhang, Chao and Yang, Carl},
  booktitle={Proceedings of the 20th IEEE International Symposium on Biomedical Imaging}, 
  selected={false},
  year={2023},
  arxiv={2211.00261},
  pdf = {https://ieeexplore.ieee.org/abstract/document/10230429}
}


@inproceedings{cui2023a,
title={A Survey on Knowledge Graphs for Healthcare: Resources, Application Progress, and Promise},
author={Hejie Cui and Jiaying Lu and Shiyu Wang and Ran Xu and Wenjing Ma and Shaojun Yu and Yue Yu and Xuan Kan and Tianfan Fu and Chen Ling and Joyce Ho and Fei Wang and Carl Yang},
booktitle={ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH)},
abbr={ICML (IMLH Workshop)},
year={2023},
pdf={https://openreview.net/forum?id=CZCktJoBRh},
arxiv={2306.04802}
}

@InProceedings{pmlr-v193-xu22a,
  abbr={ML4H},
  selected={true},
  title = 	 {Counterfactual and Factual Reasoning over Hypergraphs for Interpretable Clinical Predictions on EHR},
  author =       {Xu, Ran and Yu, Yue and Zhang, Chao and Ali, Mohammed K and Ho, Joyce C and Yang, Carl},
  booktitle = 	 {Proceedings of the 2nd Machine Learning for Health symposium},
  pages = 	 {259--278},
  year = 	 {2022},
  editor = 	 {Parziale, Antonio and Agrawal, Monica and Joshi, Shalmali and Chen, Irene Y. and Tang, Shengpu and Oala, Luis and Subbaswamy, Adarsh},
  volume = 	 {193},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28 Nov},
  publisher =    {PMLR},
  best={true},
  pdf = 	 {https://proceedings.mlr.press/v193/xu22a/xu22a.pdf},
  url = 	 {https://proceedings.mlr.press/v193/xu22a.html},
  code = {https://github.com/ritaranx/CACHE},
  abstract = 	 {Electronic Health Record modeling is crucial for digital medicine. However, existing models ignore higher-order interactions among medical codes and their causal relations towards downstream clinical predictions. To address such limitations, we propose a novel framework CACHE, to provide <em>effective</em> and <em>insightful</em> clinical predictions based on hypergraph representation learning and counterfactual and factual reasoning techniques. Experiments on two real EHR datasets show the superior performance of CACHE. Case studies with a domain expert illustrate a preferred capability of CACHE in generating clinically meaningful interpretations towards the correct predictions.}
}


}